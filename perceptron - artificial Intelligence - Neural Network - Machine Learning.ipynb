{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The perceptron is a binary classifier that _attempts_ to find an $M$ dimensional hyperplane $w$ which separates the data points $X$ into two categories $\\{0,1\\}$, given by the ground truth $y$.\n",
    "\n",
    "$\\mathbf{f}(x)= \\hat y = \n",
    "\\begin{cases}\n",
    "    1, & \\text{if }w \\cdot x + b > 0 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "Finding out the hyperplane or $w$, the perceptron training algorithm, such as Adaline, updates $w$ by iteratively computing the classification error:  \n",
    "$w \\leftarrow w+\\Delta w$, where $\\Delta w = -\\eta \\nabla J(w)$  and $\\nabla J(w)$ is the cost function and $\\eta$ is the learning rate.  \n",
    "\n",
    "The gradient of $J$, or $\\nabla J(w)$ is computed by partial derivatives:  \n",
    "$\\frac{\\partial}{\\partial w_j}J(w) = -\\sum_i \\big(y^{(i)}-a^{(i)}\\big)x_j^{(i)}$, for $j=1,..,M$ and $i=1,..,N$\n",
    "\n",
    "Activation function $\\phi (z)$ where the net input $z$ is the linear combination of the weights that are connecting the input to the output: $z=\\sum_jw_jx_j=w^\\top x$  \n",
    "The threshold function $g(z)$ is just a 0-threshold or binary step:  \n",
    "$\\hat y = \n",
    "\\begin{cases}\n",
    "    +1, & \\text{if }g(z) \\ge 0 \\\\\n",
    "    -1, & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "In the visualization below, $M$ is $2$ as in the $x_1$ and $x_2$ axes, shown as the x and y dimensions in the plot.\n",
    "\n",
    "In this example, in order to show properly, the perceptron equation $w \\cdot x + b$, or $w^\\top x + b$ defines the hyperplane in $2$ dimensions. The hyperplane is all the points $x=(x_1,x_2)$ that make the hyperplane equation zero, i.e. $w^\\top x+b=0$\n",
    "\n",
    "In the visualization below, the parameter $\\text{theta}$ (in radians) controls the angle of the hyperplane so that the $w_1$ and $w_2$ is computed from the given $\\text{theta}$, $b$ is the hyperplane offset and all the points $x$ that make $w^\\top x + b > 0$ are shown with the marker $\\text{+}$ and all the points $x$ that make $w^\\top x + b \\leq 0$ are shown with the marker $\\text{o}$.\n",
    "\n",
    "Also shown the __normal__ of the hyperplane as red arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array, sin, cos, dot, pi, arange, linspace, meshgrid\n",
    "from ipywidgets.widgets import interactive, interact, interactive_output, HBox, VBox, FloatSlider, Label, Text, jslink\n",
    "\n",
    "# Hyperplane xw+b=0\n",
    "def hyper(x1, w, b):\n",
    "    # x1, x2 are scalar/vector, w is vector, b is scalar\n",
    "    # Find and return x2 from x1 that is on the hyperplane, i.e. solve the equation wx+b=0\n",
    "    return -(x1*w[0]+b)/w[1] if w[1] != 0 else 0\n",
    "\n",
    "# Classifier\n",
    "def perceptron(x, w, b):\n",
    "    # x, w are vectors, b is scalar\n",
    "    # Return the classification, i.e. f(x)\n",
    "    return 1 if dot(x,w)+b > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453b6bf452b14f35871c7debbf35535f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.39269908169872414, description='theta', max=1.8479956785822313, min=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# h controls the display granularity\n",
    "h = 1.7\n",
    "\n",
    "# Display x1 and x2 limits\n",
    "x1_min, x1_max = -10, 10\n",
    "x2_min, x2_max = -10, 10\n",
    "\n",
    "# Widgets in decorator\n",
    "@interact(theta=FloatSlider(value=pi/8, min=-pi/1.7, max=pi/1.7), b=(-7.,7.))\n",
    "def plot_w(theta, b):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    # theta controls the angle of the hyperplane\n",
    "    w = array([cos(theta), sin(theta)])\n",
    "    # Plot normal arrows\n",
    "    x1 = linspace(2*sin(theta)*x1_min, 2*sin(theta)*x1_max, 11)\n",
    "    x2 = hyper(x1, w, b)\n",
    "    x1_end = x1 + h*w[0]\n",
    "    x2_end = x2 + h*w[1];\n",
    "    for x1_o,x2_o,x1_e,x2_e in zip(x1,x2,x1_end,x2_end):\n",
    "        plt.arrow(x1_o, x2_o, x1_e-x1_o, x2_e-x2_o, head_width=.3, color='r')\n",
    "    # Plot the hyperplane\n",
    "    x1 = arange(x1_min-h,x1_max+h,h)\n",
    "    x2 = hyper(x1, w, b)\n",
    "    plt.plot(x1, x2)\n",
    "    plt.xlim(x1_min, x1_max); plt.ylim(x2_min, x2_max); plt.grid()\n",
    "    # Show x1 and x2 points according to their classification by the perceptron, i.e. w and b\n",
    "    x1 = arange(x1_min-h,x1_max+h,h)\n",
    "    x2 = arange(x2_min-h,x2_max+h,h)\n",
    "    x1v, x2v = meshgrid(x1, x2, sparse=False, indexing='xy')\n",
    "    p1 = [(x,y) for (x,y) in zip(x1v.flatten(), x2v.flatten()) if perceptron(array([x,y]), w, b) <= 0]\n",
    "    plt.scatter(*zip(*p1), marker='o', label='class 0')\n",
    "    p2 = [(x,y) for (x,y) in zip(x1v.flatten(), x2v.flatten()) if perceptron(array([x,y]), w, b) > 0]\n",
    "    plt.scatter(*zip(*p2), marker='+', label='class 1')\n",
    "    plt.xlabel(r'$x_1$', horizontalalignment='right', x=1.0)\n",
    "    plt.ylabel(r'$x_2$', horizontalalignment='right', y=1.0)\n",
    "    plt.legend()\n",
    "    # Hyperplane w\n",
    "    plt.title(f'$\\\\theta$={theta:.2f} $w_1$={cos(theta):.2f} $w_2$={sin(theta):.2f}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study the above widget with different $\\theta$ and $b$ values. In supervised learning, this perceptron classifier is _trained_ with the labels in $y$, to determine $w_1$, $w_2$ and $b$ by minimizing the classification error. An example training program is provided in the Textbook (Raschka, 2019) Chapter 2, section Training a perceptron model on the Iris\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
